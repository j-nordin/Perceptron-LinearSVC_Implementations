{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing linear classifiers\n",
    "\n",
    "`Group`: PA 4 43\n",
    "\n",
    "`Date`: 22 February 2023\n",
    "\n",
    "`Group member`: \n",
    "- Albin Ekstr√∂m\n",
    "- Jonas Nordin"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise question\n",
    "\n",
    "The Linear Support Vector Classifier (L-SVC) is based on linear dependence. This means that e.g. a XOR-function can't be memorized by the classifier. It's the same case in the exercise question, the second training set is linearly insepreable, hence it can not be memorized. \n",
    "\n",
    "For example, if we try to separate rain and sun using only the city feature, we would have to draw a vertical line between Sydney and Paris, but this line would not separate the rain and sun classes correctly. Similarly, if we try to separate the classes using only the month feature, we would have to draw a horizontal line between July and December, but this line would also be incorrect."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aml_perceptron import LinearClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Pegasos implementation\n",
    "class LinearSVC(LinearClassifier):\n",
    "    \"\"\"\n",
    "    An implementation of a linear SVC algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=20, regularization_factor=0.001):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.regularization_factor = regularization_factor\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the pegasos algorithm.\n",
    "        \"\"\"\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "        XY = list(zip(X, Ye))\n",
    "\n",
    "        t = 1\n",
    "        for i in range(self.n_iter):\n",
    "            for x, y in XY:\n",
    "                n = 1/(self.regularization_factor * t)\n",
    "\n",
    "                # Compute the output score for this instance.\n",
    "                score = x.dot(self.w)\n",
    "\n",
    "                # Update the weights.\n",
    "                if y*score < 1:\n",
    "                    self.w = (1-n*self.regularization_factor)*self.w + (n*y)*x\n",
    "                else:\n",
    "                    self.w = (1-n*self.regularization_factor)*self.w\n",
    "                t += 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Logisitic regression\n",
    "from aml_perceptron import LinearClassifier\n",
    "\n",
    "class LogisticRegression(LinearClassifier):\n",
    "    \"\"\"\n",
    "    An implementation of a logistic regression algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=20, regularization_factor=0.001):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.regularization_factor = regularization_factor\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "            \"\"\"\n",
    "            Train a logistic regression.\n",
    "            \"\"\"\n",
    "            # First determine which output class will be associated with positive\n",
    "            # and negative scores, respectively.\n",
    "            self.find_classes(Y)\n",
    "\n",
    "            # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "            Ye = self.encode_outputs(Y)\n",
    "\n",
    "            # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "            # into a normal NumPy matrix.\n",
    "            if not isinstance(X, np.ndarray):\n",
    "                X = X.toarray()\n",
    "\n",
    "            # Initialize the weight vector to all zeros.\n",
    "            n_features = X.shape[1]\n",
    "            self.w = np.zeros(n_features)\n",
    "\n",
    "            XY = list(zip(X, Ye))\n",
    "\n",
    "            t = 1\n",
    "            for i in range(self.n_iter):\n",
    "                loss_values = []\n",
    "                for x, y in XY:\n",
    "                    n = 1/(self.regularization_factor * t)\n",
    "\n",
    "                    # Compute gradient for the loss function\n",
    "                    loss_grad = -(y / (1 + np.exp(y* (self.w.dot(x)) )))*x\n",
    "\n",
    "                    # Compute loss function value\n",
    "                    loss = np.log(1 + np.exp(-y * (self.w.dot(x))))\n",
    "                    if loss == np.inf: # If loss is too big just set it to a large number\n",
    "                        loss = 1_000_000\n",
    "                    loss_values.append(loss)\n",
    "\n",
    "                    # Compute gradient for the objective function\n",
    "                    obj_grad = self.regularization_factor * self.w + loss_grad\n",
    "\n",
    "                    # Update the weight-vector\n",
    "                    self.w = (1-n*self.regularization_factor)*self.w - n * obj_grad\n",
    "\n",
    "                    t += 1\n",
    "\n",
    "                # Print the objective function value\n",
    "                obj = np.average(loss_values) + self.regularization_factor*((self.w.dot(self.w))**2)\n",
    "                print(f\"Epoch: {i} - Objective function value: {obj}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus task 1. Making your code more efficient\n",
    "\n",
    "Here we're trying to optimize the training algorithms to run faster. We should get the same results with and without the speed improvements. The time measurements and score are presented under the section \"Evaluating the classifiers\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Faster linear algebra operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pegasos implementation optimized with faster linear algebra\n",
    "from scipy.linalg import blas\n",
    "\n",
    "class LinAlgLinearSVC(LinearClassifier):\n",
    "    \"\"\"\n",
    "    Origninal linear SVC algorithm took 1.69s to run\n",
    "    This version runs the code in \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=20, regularization_factor=0.001):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.regularization_factor = regularization_factor\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the pegasos algorithm.\n",
    "        \"\"\"\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features, dtype = np.float64)\n",
    "\n",
    "        # Initialize the X and Y as floats\n",
    "        X = X*1.0\n",
    "        Ye = Ye*1.0\n",
    "\n",
    "        # Zip the file before to save process time\n",
    "        XY = list(zip(X, Ye))\n",
    "\n",
    "        t = 1\n",
    "        for i in range(self.n_iter):\n",
    "            for x, y in XY:\n",
    "                n = 1/(self.regularization_factor * t)\n",
    "                constant = (1-n*self.regularization_factor)\n",
    "\n",
    "                # Compute the output score with the ddot function from BLAS\n",
    "                score = blas.ddot(x, self.w)\n",
    "                \n",
    "                # Update the weights.\n",
    "                if y*score < 1:\n",
    "                    blas.dscal(constant, self.w) # w = (1 - n * lambda) * w\n",
    "                    blas.daxpy(x, self.w, a=(n*y)) # w = w + x * (n*y)\n",
    "                else:\n",
    "                    blas.dscal(constant, self.w) # w = (1 - n * lambda) * w\n",
    "                t += 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Using sparse vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aml_perceptron import sparse_dense_dot, add_sparse_to_dense\n",
    "# Pegasos implementation optimized with sparse vectors\n",
    "\n",
    "class SparseLinearSVC(LinearClassifier):\n",
    "\n",
    "    def __init__(self, n_iter=20, regularization_factor=0.001):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.regularization_factor = regularization_factor\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the pegasos algorithm.\n",
    "        \"\"\"\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "        # Iteration through sparse matrices can be a bit slow, so we first\n",
    "        # prepare this list to speed up iteration.\n",
    "        XY = list(zip(X, Ye))\n",
    "\n",
    "        t = 1\n",
    "        for i in range(self.n_iter):\n",
    "            for x, y in XY:\n",
    "                n = 1/(self.regularization_factor * t)\n",
    "\n",
    "                # Compute the output score for this instance.\n",
    "                score = sparse_dense_dot(x, self.w)\n",
    "\n",
    "                # Update the weights.\n",
    "                if y*score < 1:\n",
    "                    self.w = (1-n*self.regularization_factor)*self.w\n",
    "                    add_sparse_to_dense(x, self.w, (n*y))\n",
    "                else:\n",
    "                    self.w = (1-n*self.regularization_factor)*self.w\n",
    "                t += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Speeding up the scaling operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pegasos implementation optimized with sparse vectors scaling operation\n",
    "\n",
    "class SparseScalingLinearSVC(LinearClassifier):\n",
    "\n",
    "    def __init__(self, n_iter=20, regularization_factor=0.001):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.regularization_factor = regularization_factor\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the pegasos algorithm.\n",
    "        \"\"\"\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "        # Iteration through sparse matrices can be a bit slow, so we first\n",
    "        # prepare this list to speed up iteration.\n",
    "        XY = list(zip(X, Ye))\n",
    "\n",
    "        # Intilize the step length t to 2 to avoid zero dividing\n",
    "        t = 2\n",
    "\n",
    "        # Initialize scaling factor a to 1\n",
    "        a = 1\n",
    "        for i in range(self.n_iter):\n",
    "            for x, y in XY:\n",
    "                n = 1/(self.regularization_factor * t)\n",
    "\n",
    "                # Compute the output score for this instance.\n",
    "                score = a * sparse_dense_dot(x, self.w)\n",
    "\n",
    "                # Compute the scaling factor\n",
    "                a = (1-n*self.regularization_factor)*a\n",
    "\n",
    "                # Update the weights.\n",
    "                if y * score < 1:\n",
    "                    scaling = ((n*y)/a)\n",
    "                    add_sparse_to_dense(x, self.w, scaling)\n",
    "                t += 1\n",
    "        self.w = a * self.w"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVC classifier: \n",
      "Training time: 1.54 sec.\n",
      "Accuracy: 0.8351.\n",
      "\n",
      "==========================================\n",
      "\n",
      "Linear SVC classifier with fast linear algebra: \n",
      "Training time: 1.22 sec.\n",
      "Accuracy: 0.8351.\n",
      "\n",
      "==========================================\n",
      "\n",
      "Linear SVC classifier with using sparse vectors: \n",
      "Training time: 30.85 sec.\n",
      "Accuracy: 0.8695.\n",
      "\n",
      "==========================================\n",
      "\n",
      "Linear SVC classifier with using sparse vectors and scaling operation: \n",
      "Training time: 4.72 sec.\n",
      "Accuracy: 0.8703.\n",
      "\n",
      "==========================================\n",
      "\n",
      "Logistic Regression: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n7/zkztf1z93sb1wpcbr3wrk07c0000gn/T/ipykernel_21365/3845729332.py:49: RuntimeWarning: overflow encountered in exp\n",
      "  loss = np.log(1 + np.exp(-y * (self.w.dot(x))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 - Objective function value: 275.08853119169464\n",
      "Epoch: 1 - Objective function value: 51.38838675167968\n",
      "Epoch: 2 - Objective function value: 48.22398057839656\n",
      "Epoch: 3 - Objective function value: 46.85975799486828\n",
      "Epoch: 4 - Objective function value: 46.116493261685505\n",
      "Epoch: 5 - Objective function value: 45.655410353972044\n",
      "Epoch: 6 - Objective function value: 45.343953770381106\n",
      "Epoch: 7 - Objective function value: 45.12043130366114\n",
      "Epoch: 8 - Objective function value: 44.95260141006971\n",
      "Epoch: 9 - Objective function value: 44.822120370532886\n",
      "Training time: 2.30 sec.\n",
      "Accuracy: 0.8292.\n",
      "\n",
      "==========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from aml_perceptron import Perceptron, SparsePerceptron\n",
    "\n",
    "# This function reads the corpus, returns a list of documents, and a list\n",
    "# of their corresponding polarity labels. \n",
    "def read_data(corpus_file):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(corpus_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            _, y, _, x = line.split(maxsplit=3)\n",
    "            X.append(x.strip())\n",
    "            Y.append(y)\n",
    "    return X, Y\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # Read all the documents.\n",
    "    X, Y = read_data('data/all_sentiment_shuffled.txt')\n",
    "    \n",
    "    # Split into training and test parts.\n",
    "    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "\n",
    "    # Set up the preprocessing steps and the classifier.\n",
    "    pipeline_LSVC = make_pipeline(\n",
    "        TfidfVectorizer(),\n",
    "        SelectKBest(k=1000),\n",
    "        Normalizer(),\n",
    "\n",
    "        LinearSVC(n_iter=10, regularization_factor=0.0001)\n",
    "    )\n",
    "\n",
    "    pipeline_LINALG = make_pipeline(\n",
    "        TfidfVectorizer(),\n",
    "        SelectKBest(k=1000),\n",
    "        Normalizer(),\n",
    "\n",
    "        LinAlgLinearSVC(n_iter=10, regularization_factor=0.0001)\n",
    "    )\n",
    "\n",
    "    pipeline_SPARE = make_pipeline(\n",
    "        TfidfVectorizer(ngram_range=(1,2)),\n",
    "        #SelectKBest(k=1000),\n",
    "        Normalizer(),\n",
    "\n",
    "        SparseLinearSVC(n_iter=10, regularization_factor=0.0001)\n",
    "    )\n",
    "\n",
    "    pipeline_SCALED = make_pipeline(\n",
    "        TfidfVectorizer(ngram_range=(1,2)),\n",
    "        #SelectKBest(k=1000),\n",
    "        Normalizer(),\n",
    "        \n",
    "        SparseScalingLinearSVC(n_iter=10, regularization_factor=0.0001)\n",
    "    )\n",
    "\n",
    "    pipeline_LR = make_pipeline(\n",
    "        TfidfVectorizer(),\n",
    "        SelectKBest(k=1000),\n",
    "        Normalizer(),\n",
    "    \n",
    "        LogisticRegression(n_iter=10, regularization_factor=0.0001)\n",
    "    )\n",
    "\n",
    "\n",
    "    print(\"Linear SVC classifier: \")\n",
    "\n",
    "    # Train the classifier.\n",
    "    t0 = time.time()\n",
    "    pipeline_LSVC.fit(Xtrain, Ytrain)\n",
    "    t1 = time.time()\n",
    "    print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "    # Evaluate on the test set.\n",
    "    Yguess = pipeline_LSVC.predict(Xtest)\n",
    "    print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))\n",
    "\n",
    "    print()\n",
    "    print(\"==========================================\")\n",
    "    print()\n",
    "\n",
    "    print(\"Linear SVC classifier with fast linear algebra: \")\n",
    "\n",
    "    # Train the classifier.\n",
    "    t0 = time.time()\n",
    "    pipeline_LINALG.fit(Xtrain, Ytrain)\n",
    "    t1 = time.time()\n",
    "    print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "    # Evaluate on the test set.\n",
    "    Yguess = pipeline_LINALG.predict(Xtest)\n",
    "    print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))\n",
    "\n",
    "    print()\n",
    "    print(\"==========================================\")\n",
    "    print()\n",
    "\n",
    "    print(\"Linear SVC classifier with using sparse vectors: \")\n",
    "\n",
    "    # Train the classifier.\n",
    "    t0 = time.time()\n",
    "    pipeline_SPARE.fit(Xtrain, Ytrain)\n",
    "    t1 = time.time()\n",
    "    print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "    # Evaluate on the test set.\n",
    "    Yguess = pipeline_SPARE.predict(Xtest)\n",
    "    print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))\n",
    "\n",
    "    print()\n",
    "    print(\"==========================================\")\n",
    "    print()\n",
    "\n",
    "    print(\"Linear SVC classifier with using sparse vectors and scaling operation: \")\n",
    "\n",
    "    # Train the classifier.\n",
    "    t0 = time.time()\n",
    "    pipeline_SCALED.fit(Xtrain, Ytrain)\n",
    "    t1 = time.time()\n",
    "    print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "    # Evaluate on the test set.\n",
    "    Yguess = pipeline_SCALED.predict(Xtest)\n",
    "    print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))\n",
    "\n",
    "    print()\n",
    "    print(\"==========================================\")\n",
    "    print()\n",
    "\n",
    "    print(\"Logistic Regression: \")\n",
    "\n",
    "    # Train the classifier.\n",
    "    t0 = time.time()\n",
    "    pipeline_LR.fit(Xtrain, Ytrain)\n",
    "    t1 = time.time()\n",
    "    print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "    # Evaluate on the test set.\n",
    "    Yguess = pipeline_LR.predict(Xtest)\n",
    "    print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))\n",
    "\n",
    "    print()\n",
    "    print(\"==========================================\")\n",
    "    print()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
